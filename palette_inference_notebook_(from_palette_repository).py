# -*- coding: utf-8 -*-
"""Palette inference notebook (from Palette repository).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EFfTz5gRKPgYDyOhFNJ6y15Wj8dMSoNE

## Image inpainting
A demo for image inpainting on Celeba-HQ based on [Palette](https://github.com/Janspiry/Palette-Image-to-Image-Diffusion-Models).

## Setup

### Get the GPU

*   Turn on hardware acceleration under `Runtime -> Change Runtime Type -> Hardware accelerator -> GPU`
*   Use this command to ensure that the connected machine has a GPU:
"""

!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv

"""### Clone repo"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!git clone https://github.com/fenglinglwb/MAT.git

"""### Install requirements"""

!pip install https://github.com/podgorskiy/dnnlib/releases/download/0.0.1/dnnlib-0.0.1-py3-none-any.whl

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/MAT
!pip install -r requirements.txt
# The default environment can working properly

!pip install scipy==1.13.0

import scipy
scipy.__version__

!python evaluatoin/cal_fid_pids_uids.py

!pip install lpips

!pip install https://github.com/podgorskiy/dnnlib/releases/download/0.0.1/dnnlib-0.0.1-py3-none-any.whl

!python evaluatoin/cal_fid_pids_uids.py

"""### Prepare pertrained-model
Two ways to prepare your model are provided below.

#### Download from the google driver
1. Go [here](https://drive.google.com/drive/folders/13YZ2UAmGJ-b7DICr-FDAPM7gctreJEoH?usp=sharing), right click on `.pth` file, and hit `Make a copy`. Next, locate the files in your own google drive, and rename them from `Copy of 200_Network.pth` to `200_Network.pth`. Note, `.state` file is not required for the simple inference.
2. Next, right click on each file, and hit `Get link`. Click on `Restricted`, and change it to `Anyone with the link...`, as `Viewer`. Now, copy the links. They should look something like this: `https://drive.google.com/file/d/<file id>/view?usp=sharing`. Copy the file id, and paste them into the `gdown` commands below.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Palette-Image-to-Image-Diffusion-Models/
!gdown --id 1gzNs8_9LuysjbWsHftXdHp0jDYOyIgY1

"""#### Upload from the localhost
If `gdown` command did not work, you can manually upload files for the pre-trained model.
1. Download the [files](https://drive.google.com/drive/folders/13YZ2UAmGJ-b7DICr-FDAPM7gctreJEoH?usp=sharing) to localhost.
2. Click the `files` button on the left hand side of the screen, and find the folder labled `Palette-Image-to-Image-Diffusion-Models`. Click the three dots on the right of the folder, and click `upload`. Then, select the model files that you downloaded.

### Edit configure file
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Palette-Image-to-Image-Diffusion-Models/config/

"""#### Create patchfile

Create patchfile, it will update the configuration file to inference for centering mask.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile inpainting_celebahq.patch
# --- a/config/inpainting_celebahq.json
# +++ b/config/inpainting_celebahq.json
# @@ -10,7 +10,7 @@
#          "tb_logger": "tb_logger", // path of tensorboard logger
#          "results": "results",
#          "checkpoint": "checkpoint",
# -        "resume_state": "experiments/train_inpainting_celebahq_220426_233652/checkpoint/190"
# +        "resume_state": "200"
#          // "resume_state": null // ex: 100, loading .state  and .pth from given epoch and iteration
#      },
# 
# @@ -48,7 +48,7 @@
#              "which_dataset": {
#                  "name": "InpaintDataset", // import Dataset() class / function(not recommend) from default file
#                  "args":{
# -                    "data_root": "datasets/celebahq/flist/test.flist",
# +                    "data_root": "input",
#                      "mask_config": {
#                          "mask_mode": "center"
#                      }
# @@ -56,8 +56,8 @@
#              },
#              "dataloader":{
#                  "args":{
# -                    "batch_size": 8,
# -                    "num_workers": 4,
# +                    "batch_size": 1,
# +                    "num_workers": 1,
#                      "pin_memory": true
#                  }
#              }
# --

"""#### Apply the patch file
The error `patch unexpectedly ends in middle of line` can be ignored.
"""

!apt-get install dos2unix
!dos2unix inpainting_celebahq.json
!patch < inpainting_celebahq.patch

"""### Upload test data
Upload the 256*256 face images.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Palette-Image-to-Image-Diffusion-Models/
!mkdir -p input
# %cd /content/Palette-Image-to-Image-Diffusion-Models/input/
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  savefile = open(fn, 'wb')
  savefile.write(uploaded[fn])
  print('Successfully uploaded "{}" ({} bytes).'.format(fn, len(uploaded[fn])))
  savefile.close()

"""## Inference
It will cost several minutes depending on allocated GPU.

Output images will be located under ./experiments/\<some folder\>/results/test.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Palette-Image-to-Image-Diffusion-Models/
!python run.py -c config/inpainting_celebahq.json -p test